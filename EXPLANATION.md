# SnapChef - Technical Explanation

## 1. Agent Workflow

SnapChef processes user input through a sequential pipeline with three main stages:

### Step-by-Step Process:

**1. Receive User Input**
- User uploads a fridge/pantry image via file upload widget
- User provides text constraints (e.g., "low-carb dinner", "quick lunch under $10")

**2. Retrieve Relevant Memory**
- [Not Available] - Current implementation uses session-only memory with no retrieval mechanism

**3. Plan Sub-Tasks**
- Agent receives detected ingredients + user constraints
- Internally plans to generate 3 recipe options with different optimization goals:
  - **Sub-task A**: Generate fastest recipe (minimize prep time)
  - **Sub-task B**: Generate highest waste-reduction recipe (maximize ingredient usage)
  - **Sub-task C**: Generate budget-friendly recipe (minimize missing items)
- All sub-tasks executed in a single Gemini API call with comprehensive prompt

**4. Call Tools/APIs**
- **Gemini API**: Vision analysis (multimodal input)
- **Gemini API**: Recipe generation (text generation with JSON schema)
- **Unsplash Source API**: Fetch recipe images based on dish names

**5. Summarize and Return Final Output**
- Parse JSON response containing 3 recipe options
- Display interactive HTML cards with images
- User selects preferred option
- Display full recipe with instructions and shopping list
- Offer export options (email or copy/paste)

---

## 2. Key Modules

### Vision Module (`detect_ingredients()`)

**Purpose**: Analyzes fridge images using Gemini's multimodal capabilities

**Key Logic**:
```python
def detect_ingredients(image, additional_context=""):
    model = genai.GenerativeModel('models/gemini-2.5-flash')
    prompt = """
    Analyze this fridge/pantry image.
    Identify ALL visible ingredients with quantities.
    Organize by category.
    """
    response = model.generate_content([prompt, image])
    return response.text
```

**Input**: PIL Image object  
**Output**: Text string with categorized ingredients

---

### Recipe Planner (`generate_recipe_options()`)

**Purpose**: Generates 3 diverse recipe options optimized for different goals

**Key Logic**:
```python
def generate_recipe_options(detected_ingredients, user_constraints):
    prompt = f"""
    Create 3 DIFFERENT recipes using: {detected_ingredients}
    Constraints: {user_constraints}
    Exclude basic staples (salt, oil, water).
    Return as JSON with specific schema.
    """
    response = model.generate_content(prompt)
    return json.loads(clean_response(response.text))
```

**Input**: Ingredient list (string) + constraints (string)  
**Output**: List of 3 recipe dictionaries

---

### Display Executor (`display_recipe_options()`, `display_full_recipe()`)

**Purpose**: Renders recipes in user-friendly HTML/text formats

**Key Features**:
- HTML/CSS cards with Unsplash images
- Color-coded badges (prep time, calories, items needed)
- Formatted text with Unicode symbols
- Interactive user selection handling

---

### Memory Store (Session Variables)

**Type**: In-memory Python variables (no persistent storage)

**Stored State**:
```python
uploaded_image = None           # PIL.Image object
detected_ingredients = None     # String with ingredient list
generated_recipes = None        # List of 3 recipe dicts
selected_recipe = None          # Single recipe dict
```

**Limitations**: 
- No persistence across sessions
- No learning from past interactions
- Resets on kernel restart


---

## 3. Tool Integration

### Gemini API (`google.generativeai`)

**Purpose**: Vision analysis and recipe generation

**How We Call It**:
```python
# Configuration
import google.generativeai as genai
genai.configure(api_key=os.environ['GEMINI_API_KEY'])

# Vision call (multimodal)
model = genai.GenerativeModel('models/gemini-2.5-flash')
response = model.generate_content([text_prompt, image])

# Text generation call
response = model.generate_content(text_prompt)
```

**Function**: `model.generate_content()`  
**Endpoints Used**: `generateContent` for both vision and text generation

---

### Unsplash Source API

**Purpose**: Fetch recipe images for visual display

**How We Call It**:
```python
def get_recipe_image(search_query):
    base_url = "https://source.unsplash.com/400x300/"
    return f"{base_url}?{search_query},food,dish"

# Example: get_recipe_image("chicken stir fry")
```

**Function**: Direct URL construction (no SDK)  
**Authentication**: None required (public API)

---

### SMTP Email API

**Purpose**: Send shopping lists via email

**How We Call It**:
```python
import smtplib
from email.mime.text import MIMEText

server = smtplib.SMTP('smtp.gmail.com', 587)
server.starttls()
server.login(sender_email, password)
server.send_message(msg)
```

**Function**: Standard library `smtplib.SMTP()`  
**Authentication**: User provides credentials at runtime

---

## 4. Observability & Testing

### Logging

**Console Output**: All key steps print progress indicators and status messages directly to notebook output.

**Console Output**:
- Progress indicators for each step (üîç, ‚úÖ, üìã)
- Success/failure status messages
- Partial API response previews
- Error details with troubleshooting hints

**Example Log**:
```
üîç Analyzing image with Gemini Vision...
--------------------------------------------------
‚úÖ Analysis complete!
==================================================

üìã DETECTED INGREDIENTS:
Proteins: 6 eggs, 1 chicken breast
Vegetables: 2 red bell peppers...
```

---

### Error Handling

**API Failures**:
```python
try:
    response = model.generate_content([prompt, image])
except Exception as e:
    print(f"‚ùå Error: {e}")
    print("üîç Troubleshooting tips:")
```

**JSON Parsing Failures**:
```python
try:
    recipe_data = json.loads(response_text)
except json.JSONDecodeError as e:
    print(f"‚ö†Ô∏è JSON parsing error: {e}")
    print("Raw response:", response.text[:500])
```

---

### How Judges Can Trace Decisions

**Manual Testing Path**:
1. Run cells 1-3 (setup, see ‚úÖ confirmations)
2. Run cells 4-5 (upload image, see preview)
3. Run cells 6-7 (see detected ingredients printed)
4. Run cell 8 (functions defined)
5. Run cell 9 (see 3 recipe cards, make selection)
6. Run cells 10-11 (see full recipe, test export)

**Traceability**:
- All intermediate results printed inline
- User selections echoed back
- API responses partially displayed
- JSON parsing errors show raw output

---

**Validation Checklist**:
- [ ] Ingredients detected match image
- [ ] All 3 recipes respect constraints
- [ ] Shopping lists are accurate
- [ ] Cooking instructions are safe and logical
- [ ] Nutrition estimates are reasonable
- [ ] Images display correctly

### Tracing Decisions

**Step-by-step visibility**:
1. Image upload ‚Üí Shows image preview
2. Vision analysis ‚Üí Prints full ingredient list
3. Constraint input ‚Üí Echoes user input
4. Recipe generation ‚Üí Shows summary of 3 options
5. Selection ‚Üí Confirms choice
6. Full recipe ‚Üí Displays complete details

--------
**Judges can trace**:
- Which ingredients were detected
- How constraints influenced recipes
- Why certain ingredients were/weren't used
- How waste reduction was calculated


## 5. Known Limitations

### 1. Long-Running API Calls

**Issue**: Combined vision + recipe generation takes 10-15 seconds

**Breakdown**:
- Vision analysis: 3-5 seconds
- Recipe generation: 5-8 seconds
- Image fetching: 1-2 seconds

**Why**: Sequential API calls to Gemini + Unsplash

**Current Mitigation**: Show progress indicators, use Flash model (faster than Pro)

---

### 2. Handling of Ambiguous User Inputs

**Issue**: Vague constraints like "healthy meal" are subjective

**Examples**:
- "healthy" - Low-cal? High-protein? Whole foods?
- "quick" - 10 minutes? 30 minutes?
- "budget-friendly" - No specific dollar amount

**Impact**: Generated recipes may not match user's actual intent

**Current Mitigation**: AI makes reasonable assumptions, explains in chef_notes

---

### 3. Ingredient Detection Accuracy (~10% Error Rate)

**Issue**: Vision model occasionally misidentifies or misses items

**Why**:
- Blurry photos reduce accuracy
- Small items (spices, condiments) often missed
- Similar-looking items confused

**Current Mitigation**: Prompt instructs model to only include high-confidence detections

---

### 4. No Persistent Memory

**Issue**: Agent has no memory between sessions

**Impact**:
- Can't remember past preferences
- Can't learn from recipe ratings
- Repeats same questions every session

**Why**: Session-only architecture (no database, no auth system)

---

### 5. JSON Parsing Fragility

**Issue**: Gemini occasionally returns malformed JSON

**Frequency**: ~2-5% of requests

**Current Mitigation**: Strip markdown blocks, show error with raw response, user can retry

---

### 6. No Real-Time Pricing Data

**Issue**: Shopping lists show items but not actual current prices

**Impact**: User doesn't know total cost until they shop

**Status**: Price lookup via Google Search planned for future version

---

### 7. English-Only Interface

**Issue**: All prompts, recipes, and UI text in English only

**Impact**: Excludes non-English speakers

**Why**: Multilingual support requires translated prompts, different measurement systems, localized food names

---

## Performance Notes

**Typical Session Cost**: ~$0.005 per complete workflow (Gemini 2.5 Flash pricing)

**Response Times**:
- Vision analysis: 3-5 seconds
- Recipe generation: 5-8 seconds
- Total user wait time: ~10-15 seconds

**Rate Limits**: Gemini free tier allows sufficient requests for demo purposes

**Scalability**: Current architecture is single-user, synchronous. No load testing performed.
